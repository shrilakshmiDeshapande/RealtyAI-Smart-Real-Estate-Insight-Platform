{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec04816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "X_train shape: (996, 256, 256, 4) | dtype: float32 | range: [0.000, 1.000]\n",
      "y_train shape: (996, 256, 256) | dtype: float32 | unique: [0. 1.]\n",
      "X_val shape:   (249, 256, 256, 4)\n",
      "y_val shape:   (249, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "X_train = np.load('../data/spacenet/X_train.npy')\n",
    "y_train = np.load('../data/spacenet/y_train.npy')\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.load('../data/spacenet/y_val.npy')\n",
    "\n",
    "# Print shapes and stats\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"X_train shape: {X_train.shape} | dtype: {X_train.dtype} | range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"y_train shape: {y_train.shape} | dtype: {y_train.dtype} | unique: {np.unique(y_train)}\")\n",
    "print(f\"X_val shape:   {X_val.shape}\")\n",
    "print(f\"y_val shape:   {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building UNet model...\n",
      "âœ… Model built and compiled!\n",
      "Input shape: (None, 256, 256, 4)\n",
      "Output shape: (None, 256, 256, 1)\n",
      "Total parameters: 471,841\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- Custom Metrics (IoU & Dice) ---\n",
    "def iou(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Threshold at 0.5\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    return tf.reduce_mean((intersection + smooth) / (union + smooth))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    denom = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    return tf.reduce_mean((2. * intersection + smooth) / (denom + smooth))\n",
    "\n",
    "# --- Build Lightweight UNet ---\n",
    "def build_unet(input_shape=(256, 256, 4)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(32, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D(2)(c1)\n",
    "    \n",
    "    c2 = Conv2D(64, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D(2)(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, 3, activation='relu', padding='same')(c3)\n",
    "    \n",
    "    # Decoder\n",
    "    u4 = UpSampling2D(2)(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(64, 3, activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, 3, activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u5 = UpSampling2D(2)(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(32, 3, activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, 3, activation='relu', padding='same')(c5)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(c5)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --- Build and Compile ---\n",
    "print(\"Building UNet model...\")\n",
    "model = build_unet(input_shape=(256, 256, 4))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', iou, dice_coef]\n",
    ")\n",
    "\n",
    "print(\"âœ… Model built and compiled!\")\n",
    "print(\"Input shape:\", model.input_shape)\n",
    "print(\"Output shape:\", model.output_shape)\n",
    "print(\"Total parameters:\", f\"{model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b37041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Masks reshaped to include channel dimension!\n",
      "y_train shape: (996, 256, 256, 1)\n",
      "y_val shape: (249, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "X_train = np.load('../data/spacenet/X_train.npy')\n",
    "y_train = np.load('../data/spacenet/y_train.npy')\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.load('../data/spacenet/y_val.npy')\n",
    "\n",
    "# âœ… FIX: Add channel dimension to masks\n",
    "y_train = np.expand_dims(y_train, axis=-1)  # (996, 256, 256) â†’ (996, 256, 256, 1)\n",
    "y_val = np.expand_dims(y_val, axis=-1)      # (249, 256, 256) â†’ (249, 256, 256, 1)\n",
    "\n",
    "print(\"âœ… Masks reshaped to include channel dimension!\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1c1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "ðŸš€ Starting training...\n",
      "Epoch 1/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 1s/step - accuracy: 0.9216 - dice_coef: 0.0016 - iou: 8.9281e-04 - loss: 0.2718 - val_accuracy: 0.9199 - val_dice_coef: 1.2227e-09 - val_iou: 1.2227e-09 - val_loss: 0.2630\n",
      "Epoch 2/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.9254 - dice_coef: 1.6188e-09 - iou: 1.6188e-09 - loss: 0.2491 - val_accuracy: 0.9199 - val_dice_coef: 1.2227e-09 - val_iou: 1.2227e-09 - val_loss: 0.2564\n",
      "Epoch 3/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.9254 - dice_coef: 1.6188e-09 - iou: 1.6188e-09 - loss: 0.2483 - val_accuracy: 0.9199 - val_dice_coef: 1.2227e-09 - val_iou: 1.2227e-09 - val_loss: 0.2758\n",
      "Epoch 4/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 1s/step - accuracy: 0.9257 - dice_coef: 0.0119 - iou: 0.0065 - loss: 0.2422 - val_accuracy: 0.9205 - val_dice_coef: 0.0178 - val_iou: 0.0091 - val_loss: 0.2441\n",
      "Epoch 5/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 1s/step - accuracy: 0.9259 - dice_coef: 0.0365 - iou: 0.0205 - loss: 0.2427 - val_accuracy: 0.9199 - val_dice_coef: 3.4600e-04 - val_iou: 1.7323e-04 - val_loss: 0.2483\n",
      "Epoch 6/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 1s/step - accuracy: 0.9256 - dice_coef: 0.0199 - iou: 0.0103 - loss: 0.2377 - val_accuracy: 0.9206 - val_dice_coef: 0.0615 - val_iou: 0.0323 - val_loss: 0.2410\n",
      "Epoch 7/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.9263 - dice_coef: 0.0554 - iou: 0.0295 - loss: 0.2331 - val_accuracy: 0.9217 - val_dice_coef: 0.1074 - val_iou: 0.0583 - val_loss: 0.2371\n",
      "Epoch 8/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.9270 - dice_coef: 0.0873 - iou: 0.0474 - loss: 0.2305 - val_accuracy: 0.9224 - val_dice_coef: 0.0902 - val_iou: 0.0487 - val_loss: 0.2359\n",
      "Epoch 9/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1s/step - accuracy: 0.9280 - dice_coef: 0.1159 - iou: 0.0652 - loss: 0.2268 - val_accuracy: 0.9236 - val_dice_coef: 0.1285 - val_iou: 0.0736 - val_loss: 0.2340\n",
      "Epoch 10/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1s/step - accuracy: 0.9279 - dice_coef: 0.1140 - iou: 0.0637 - loss: 0.2259 - val_accuracy: 0.9232 - val_dice_coef: 0.1424 - val_iou: 0.0799 - val_loss: 0.2334\n",
      "Epoch 11/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 1s/step - accuracy: 0.9280 - dice_coef: 0.1266 - iou: 0.0714 - loss: 0.2236 - val_accuracy: 0.9234 - val_dice_coef: 0.1738 - val_iou: 0.1004 - val_loss: 0.2277\n",
      "Epoch 12/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 1s/step - accuracy: 0.9281 - dice_coef: 0.1341 - iou: 0.0756 - loss: 0.2219 - val_accuracy: 0.9236 - val_dice_coef: 0.1623 - val_iou: 0.0924 - val_loss: 0.2260\n",
      "Epoch 13/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1s/step - accuracy: 0.9285 - dice_coef: 0.1502 - iou: 0.0862 - loss: 0.2185 - val_accuracy: 0.9222 - val_dice_coef: 0.0728 - val_iou: 0.0397 - val_loss: 0.2408\n",
      "Epoch 14/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 1s/step - accuracy: 0.9285 - dice_coef: 0.1466 - iou: 0.0840 - loss: 0.2161 - val_accuracy: 0.9233 - val_dice_coef: 0.1864 - val_iou: 0.1080 - val_loss: 0.2242\n",
      "Epoch 15/15\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 1s/step - accuracy: 0.9283 - dice_coef: 0.1459 - iou: 0.0836 - loss: 0.2178 - val_accuracy: 0.9240 - val_dice_coef: 0.1586 - val_iou: 0.0914 - val_loss: 0.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to ../models/unet_building_segmentation.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Load and reshape data (already done, but included for completeness) ---\n",
    "X_train = np.load('../data/spacenet/X_train.npy')\n",
    "y_train = np.expand_dims(np.load('../data/spacenet/y_train.npy'), axis=-1)\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.expand_dims(np.load('../data/spacenet/y_val.npy'), axis=-1)\n",
    "\n",
    "# --- Custom Metrics ---\n",
    "def iou(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    return tf.reduce_mean((intersection + smooth) / (union + smooth))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    denom = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    return tf.reduce_mean((2. * intersection + smooth) / (denom + smooth))\n",
    "\n",
    "# --- Build Model ---\n",
    "def build_unet(input_shape=(256, 256, 4)):\n",
    "    inputs = Input(input_shape)\n",
    "    c1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(32, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D(2)(c1)\n",
    "    \n",
    "    c2 = Conv2D(64, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D(2)(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, 3, activation='relu', padding='same')(c3)\n",
    "    \n",
    "    u4 = UpSampling2D(2)(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(64, 3, activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, 3, activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u5 = UpSampling2D(2)(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(32, 3, activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, 3, activation='relu', padding='same')(c5)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(c5)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --- Compile ---\n",
    "print(\"Building model...\")\n",
    "model = build_unet()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', iou, dice_coef]\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=4,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Save ---\n",
    "model.save(\"../models/unet_building_segmentation.h5\")\n",
    "print(\"âœ… Model saved to ../models/unet_building_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bba3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample predictions...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "âœ… Sample predictions saved to ../results/segmentation_samples.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load model and data\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"../models/unet_building_segmentation.h5\", custom_objects={'iou': iou, 'dice_coef': dice_coef})\n",
    "\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.expand_dims(np.load('../data/spacenet/y_val.npy'), axis=-1)\n",
    "\n",
    "# Predict on first 3 validation samples\n",
    "print(\"Generating sample predictions...\")\n",
    "preds = model.predict(X_val[:3])\n",
    "\n",
    "# Plot\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    # Original RGB\n",
    "    plt.subplot(3, 3, i*3 + 1)\n",
    "    plt.imshow(X_val[i][:,:,:3])\n",
    "    plt.title(f\"Input {i+1} (RGB)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    plt.subplot(3, 3, i*3 + 2)\n",
    "    plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction (thresholded)\n",
    "    plt.subplot(3, 3, i*3 + 3)\n",
    "    plt.imshow((preds[i].squeeze() > 0.5), cmap='gray')\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/segmentation_samples.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"âœ… Sample predictions saved to ../results/segmentation_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bd6a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on full validation set...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.9240 - dice_coef: 0.1588 - iou: 0.0916 - loss: 0.2196\n",
      "\n",
      " FINAL TEST RESULTS:\n",
      "  IoU:        0.0916\n",
      "  Dice:       0.1588\n",
      "  Accuracy:   0.9240\n",
      "  Loss:       0.2196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model (with custom metrics)\n",
    "model = load_model(\n",
    "    \"../models/unet_building_segmentation.h5\",\n",
    "    custom_objects={'iou': iou, 'dice_coef': dice_coef}\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.expand_dims(np.load('../data/spacenet/y_val.npy'), axis=-1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating on full validation set...\")\n",
    "metrics = model.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "# Extract results\n",
    "loss, accuracy, iou_score, dice_score = metrics\n",
    "print(f\"\\n FINAL TEST RESULTS:\")\n",
    "print(f\"  IoU:        {iou_score:.4f}\")\n",
    "print(f\"  Dice:       {dice_score:.4f}\")\n",
    "print(f\"  Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"  Loss:       {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9577a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation saved to ../results/segmentation_evaluation.txt\n"
     ]
    }
   ],
   "source": [
    "# Save metrics to file\n",
    "with open(\"../results/segmentation_evaluation.txt\", \"w\") as f:\n",
    "    f.write(\"Week 3: Image Segmentation Evaluation\\n\")\n",
    "    f.write(f\"IoU: {iou_score:.4f}\\n\")\n",
    "    f.write(f\"Dice: {dice_score:.4f}\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "print(\" Evaluation saved to ../results/segmentation_evaluation.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14247ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "âœ… Qualitative plot saved: segmentation_qualitative.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model and data\n",
    "model = load_model(\"../models/unet_building_segmentation.h5\", custom_objects={'iou': iou, 'dice_coef': dice_coef})\n",
    "X_val = np.load('../data/spacenet/X_val.npy')\n",
    "y_val = np.expand_dims(np.load('../data/spacenet/y_val.npy'), axis=-1)\n",
    "\n",
    "# Select 4 diverse samples\n",
    "indices = [10, 50, 100, 150]  # rural, suburban, urban, complex\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, idx in enumerate(indices):\n",
    "    pred = model.predict(X_val[idx:idx+1])[0]\n",
    "    pred_mask = (pred.squeeze() > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # RGB Input\n",
    "    plt.subplot(4, 3, i*3 + 1)\n",
    "    plt.imshow(X_val[idx][:,:,:3])\n",
    "    plt.title(f\"Input {i+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    plt.subplot(4, 3, i*3 + 2)\n",
    "    plt.imshow(y_val[idx].squeeze(), cmap='gray')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    plt.subplot(4, 3, i*3 + 3)\n",
    "    plt.imshow(pred_mask, cmap='gray')\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/segmentation_qualitative.png\", dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… Qualitative plot saved: segmentation_qualitative.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e00bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics plot saved: segmentation_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# Assuming you saved history earlier (if not, retrain briefly or skip)\n",
    "# If you have history.history, use it. Otherwise, create a mock-up from your logs:\n",
    "\n",
    "epochs = list(range(1, 16))\n",
    "train_iou = [0.0009, 0.0000, 0.0000, 0.0065, 0.0205, 0.0103, 0.0295, 0.0474, 0.0652, 0.0637, 0.0714, 0.0756, 0.0862, 0.0840, 0.0836]\n",
    "val_iou = [0.0000, 0.0000, 0.0000, 0.0091, 0.0002, 0.0323, 0.0583, 0.0487, 0.0736, 0.0799, 0.1004, 0.0924, 0.0397, 0.1080, 0.0914]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_iou, 'b-o', label='Train IoU')\n",
    "plt.plot(epochs, val_iou, 'r-o', label='Val IoU')\n",
    "plt.title('IoU Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Dice (approximate from IoU: Dice â‰ˆ 2*IoU/(1+IoU))\n",
    "train_dice = [2*i/(1+i) for i in train_iou]\n",
    "val_dice = [2*i/(1+i) for i in val_iou]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_dice, 'b-o', label='Train Dice')\n",
    "plt.plot(epochs, val_dice, 'r-o', label='Val Dice')\n",
    "plt.title('Dice Coefficient Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/segmentation_metrics.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"âœ… Metrics plot saved: segmentation_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9610f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary plot saved: segmentation_summary.png\n"
     ]
    }
   ],
   "source": [
    "# Final metrics from your evaluation\n",
    "metrics = {\n",
    "    'IoU': 0.0916,\n",
    "    'Dice': 0.1588,\n",
    "    'Accuracy': 0.9240\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(metrics.keys(), metrics.values(), color=['steelblue', 'crimson', 'seagreen'])\n",
    "plt.title('Week 3: Segmentation Model Performance')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/segmentation_summary.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"âœ… Summary plot saved: segmentation_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866969fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
